{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d1b8f91-992a-42ab-a220-fc0ee96e8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # console progress bar\n",
    "import numpy as np\n",
    "\n",
    "OLLAMA_HOST = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3.1\")\n",
    "\n",
    "USE_CONTEXT = False          \n",
    "ACCEPT_THRESHOLD = 0.0       \n",
    "REQUEST_TIMEOUT_S = 30       \n",
    "MAX_RETRIES = 2              \n",
    "HEARTBEAT_EVERY = 25         \n",
    "\n",
    "def _clean(s: Optional[str]) -> str:\n",
    "    return (s or \"\").strip()\n",
    "\n",
    "def _to_list(x: Any) -> List[Any]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return [x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dff4c981-05f5-40b8-8b4d-1cee6db08590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load a dataset file that is either:\n",
    "      - a single JSON object,\n",
    "      - a list of JSON objects,\n",
    "      - or a JSONL file (one object per line).\n",
    "    Returns a list of objects.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        rows = []\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    rows.append(json.loads(line))\n",
    "        return rows\n",
    "    else:\n",
    "        obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "        return [obj]\n",
    "\n",
    "def extract_targets(obj: Dict[str, Any]) -> List[str]:\n",
    "    # \"target\" is a list of acceptable strings\n",
    "    return [_clean(t) for t in _to_list(obj.get(\"target\", [])) if _clean(t)]\n",
    "\n",
    "def extract_candidates(obj: Dict[str, Any]) -> List[str]:\n",
    "    # \"clarified_all_ans\" is a List[List[str]]; flatten it\n",
    "    out: List[str] = []\n",
    "    blocks = _to_list(obj.get(\"clarified_all_ans\", []))\n",
    "    for block in blocks:\n",
    "        for s in _to_list(block):\n",
    "            s_clean = _clean(s)\n",
    "            if s_clean:\n",
    "                out.append(s_clean)\n",
    "    return out\n",
    "\n",
    "def to_context(obj: Dict[str, Any]) -> str:\n",
    "    # Kept for compatibility; NOT used when USE_CONTEXT=False\n",
    "    return _clean(obj.get(\"input\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e4828f2-45a6-4468-9a29-b3c96d152b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM_PROMPT = (\n",
    "    \"You judge whether CANDIDATE is semantically equivalent to TARGET.\\n\"\n",
    "    \"Focus on meaning, not wording. Ignore extra fluff.\\n\"\n",
    "    'Return ONLY JSON: {\"equivalent\": true|false, \"rationale\": \"...\"}\\n'\n",
    "    \"- TRUE only if a grader would accept CANDIDATE in place of TARGET with no change of meaning.\\n\"\n",
    "    \"- Minor rephrasing/synonyms → may be TRUE. Broader/narrower/related terms → FALSE.\\n\"\n",
    ")\n",
    "\n",
    "SCORE_SYSTEM_PROMPT = (\n",
    "    \"You assign a semantic similarity score between TARGET and CANDIDATE.\\n\"\n",
    "    \"Focus purely on meaning (0.0 = completely different, 1.0 = identical in meaning).\\n\"\n",
    "    'Return ONLY JSON: {\"score\": 0..1, \"rationale\": \"...\"}\\n'\n",
    "    \"- 1.0 if they express the same meaning with no substantive difference.\\n\"\n",
    "    \"- ~0.7–0.9 if they are very similar but not identical.\\n\"\n",
    "    \"- ~0.3–0.6 if partially related.\\n\"\n",
    "    \"- 0.0–0.2 if mostly or completely unrelated.\\n\"\n",
    ")\n",
    "\n",
    "def build_judge_prompt(target: str, candidate: str, context: str = \"\", use_context: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Builds the user message. If use_context=False, context is ignored.\n",
    "    \"\"\"\n",
    "    if use_context:\n",
    "        return (\n",
    "            \"CONTEXT:\\n{ctx}\\n\\nTARGET:\\n{tgt}\\n\\nCANDIDATE:\\n{cand}\\n\\n\"\n",
    "            \"Decide if CANDIDATE expresses the same meaning as TARGET in this context.\"\n",
    "        ).format(ctx=context, tgt=target, cand=candidate)\n",
    "    else:\n",
    "        return (\n",
    "            \"TARGET:\\n{tgt}\\n\\nCANDIDATE:\\n{cand}\\n\\n\"\n",
    "            \"Decide if CANDIDATE expresses the same meaning as TARGET.\"\n",
    "        ).format(tgt=target, cand=candidate)\n",
    "\n",
    "\n",
    "def build_score_prompt(target: str, candidate: str, context: str = \"\", use_context: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Builds the user message for scoring. If use_context=False, context is ignored.\n",
    "    \"\"\"\n",
    "    if use_context:\n",
    "        return (\n",
    "            \"CONTEXT:\\n{ctx}\\n\\nTARGET:\\n{tgt}\\n\\nCANDIDATE:\\n{cand}\\n\\n\"\n",
    "            \"Assign a semantic similarity score (0.0–1.0) based on their meanings in this context.\"\n",
    "        ).format(ctx=context, tgt=target, cand=candidate)\n",
    "    else:\n",
    "        return (\n",
    "            \"TARGET:\\n{tgt}\\n\\nCANDIDATE:\\n{cand}\\n\\n\"\n",
    "            \"Assign a semantic similarity score (0.0–1.0) based on their meanings.\"\n",
    "        ).format(tgt=target, cand=candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf83be22-5faf-4168-98cd-be6864f91ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _post_chat(model: str, system: str, user: str, host: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Calls Ollama /api/chat and returns the assistant message content as a string.\n",
    "    Raises on hard errors; caller handles retries.\n",
    "    \"\"\"\n",
    "    url = host.rstrip(\"/\") + \"/api/chat\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                url,\n",
    "                json={\n",
    "                    \"model\": model,\n",
    "                    \"messages\": messages,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": temperature},\n",
    "                },\n",
    "                timeout=REQUEST_TIMEOUT_S,\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            content = (resp.json().get(\"message\", {}) or {}).get(\"content\", \"\")\n",
    "            return _clean(content)\n",
    "        except Exception as e:\n",
    "            if attempt == MAX_RETRIES:\n",
    "                raise\n",
    "            time.sleep(1.0)\n",
    "\n",
    "def _parse_json_from_content(content: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts the first {...} block and parses it as JSON. Returns {} on failure.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\{.*\\}\", content, flags=re.DOTALL)\n",
    "    s = m.group(0) if m else content\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d535e4b-9e16-48fa-8246-056b3f1c27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _judge_equivalent_flag(\n",
    "    model: str,\n",
    "    host: str,\n",
    "    target: str,\n",
    "    candidate: str,\n",
    "    context: str = \"\",\n",
    "    use_context: bool = USE_CONTEXT,\n",
    "    temperature: float = 0.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    First call: ask the LLM only for the binary semantic equivalence decision.\n",
    "    Returns: {\"equivalent\": bool, \"rationale\": str, \"raw\": str}\n",
    "    \"\"\"\n",
    "    user = build_judge_prompt(target=target, candidate=candidate, context=context, use_context=use_context)\n",
    "    content = _post_chat(model, JUDGE_SYSTEM_PROMPT, user, host, temperature=temperature)\n",
    "    data = _parse_json_from_content(content)\n",
    "\n",
    "    eq = bool(data.get(\"equivalent\", False))\n",
    "    rationale = _clean(data.get(\"rationale\", \"\"))\n",
    "\n",
    "    return {\"equivalent\": eq, \"rationale\": rationale, \"raw\": content}\n",
    "\n",
    "\n",
    "def _judge_similarity_score(\n",
    "    model: str,\n",
    "    host: str,\n",
    "    target: str,\n",
    "    candidate: str,\n",
    "    context: str = \"\",\n",
    "    use_context: bool = USE_CONTEXT,\n",
    "    temperature: float = 0.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Second call: ask the LLM only for the semantic similarity score.\n",
    "    Returns: {\"score\": float, \"rationale\": str, \"raw\": str}\n",
    "    \"\"\"\n",
    "    user = build_score_prompt(target=target, candidate=candidate, context=context, use_context=use_context)\n",
    "    content = _post_chat(model, SCORE_SYSTEM_PROMPT, user, host, temperature=temperature)\n",
    "    data = _parse_json_from_content(content)\n",
    "\n",
    "    try:\n",
    "        score = float(data.get(\"score\", 0.0))\n",
    "    except Exception:\n",
    "        score = 0.0\n",
    "\n",
    "    rationale = _clean(data.get(\"rationale\", \"\"))\n",
    "\n",
    "    return {\"score\": score, \"rationale\": rationale, \"raw\": content}\n",
    "\n",
    "\n",
    "def judge_equivalence_once(\n",
    "    model: str,\n",
    "    host: str,\n",
    "    target: str,\n",
    "    candidate: str,\n",
    "    context: str = \"\",\n",
    "    use_context: bool = USE_CONTEXT,\n",
    "    temperature: float = 0.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ask the LLM in two separate calls:\n",
    "      1) Decide semantic equivalence (boolean).\n",
    "      2) Provide a semantic similarity score (0..1).\n",
    "\n",
    "    Returns dict:\n",
    "      {\n",
    "        \"equivalent\": bool,         # after applying ACCEPT_THRESHOLD\n",
    "        \"score\": float,\n",
    "        \"rationale\": str,           # combined rationale (equiv + score)\n",
    "        \"raw\": str                  # JSON string with both raw responses\n",
    "      }\n",
    "    \"\"\"\n",
    "    # First: binary decision\n",
    "    eq_res = _judge_equivalent_flag(\n",
    "        model=model,\n",
    "        host=host,\n",
    "        target=target,\n",
    "        candidate=candidate,\n",
    "        context=context,\n",
    "        use_context=use_context,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    # Second: score\n",
    "    score_res = _judge_similarity_score(\n",
    "        model=model,\n",
    "        host=host,\n",
    "        target=target,\n",
    "        candidate=candidate,\n",
    "        context=context,\n",
    "        use_context=use_context,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    eq = bool(eq_res.get(\"equivalent\", False))\n",
    "    score = float(score_res.get(\"score\", 0.0))\n",
    "\n",
    "    # Optional strictness gate *now* uses the separately obtained score\n",
    "    if eq and score < ACCEPT_THRESHOLD:\n",
    "        eq = False\n",
    "\n",
    "    # Combine rationales in case you want both\n",
    "    combined_rationale_parts = []\n",
    "    if eq_res.get(\"rationale\"):\n",
    "        combined_rationale_parts.append(\"Equivalence rationale: \" + eq_res[\"rationale\"])\n",
    "    if score_res.get(\"rationale\"):\n",
    "        combined_rationale_parts.append(\"Score rationale: \" + score_res[\"rationale\"])\n",
    "    combined_rationale = \" | \".join(combined_rationale_parts)\n",
    "\n",
    "    # Pack both raw responses as JSON string (so type stays str)\n",
    "    raw_combined = json.dumps(\n",
    "        {\n",
    "            \"equivalence_call_raw\": eq_res.get(\"raw\", \"\"),\n",
    "            \"score_call_raw\": score_res.get(\"raw\", \"\"),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"equivalent\": eq,\n",
    "        \"score\": score,\n",
    "        \"rationale\": combined_rationale,\n",
    "        \"raw\": raw_combined,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87995b54-ea03-4456-b7f9-afef495de3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_equivalence_against_targets(\n",
    "    targets: List[str],\n",
    "    candidate: str,\n",
    "    model: str = OLLAMA_MODEL,\n",
    "    host: str = OLLAMA_HOST,\n",
    "    context: str = \"\",              # ignored when USE_CONTEXT=False\n",
    "    use_context: bool = USE_CONTEXT,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs judge_equivalence_once for candidate against each target; returns the best one by\n",
    "    (score, equivalent) descending.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for tgt in targets:\n",
    "        r = judge_equivalence_once(model, host, tgt, candidate, context=context, use_context=use_context)\n",
    "        r[\"target\"] = tgt\n",
    "        results.append(r)\n",
    "\n",
    "    # sort by score desc; if tie, prefer equivalent=True\n",
    "    results.sort(key=lambda d: (d.get(\"score\", 0.0), bool(d.get(\"equivalent\", False))), reverse=True)\n",
    "    best = results[0] if results else {\"equivalent\": False, \"score\": 0.0, \"rationale\": \"\", \"target\": \"\", \"raw\": \"\"}\n",
    "\n",
    "    return {\n",
    "        \"best_equivalent\": bool(best.get(\"equivalent\", False)),\n",
    "        \"best_score\": float(best.get(\"score\", 0.0)),\n",
    "        \"best_rationale\": _clean(best.get(\"rationale\", \"\")),\n",
    "        \"best_target\": _clean(best.get(\"target\", \"\")),\n",
    "        \"judge_raw\": best.get(\"raw\", \"\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8be7e76-4b45-4e68-b141-1b7da84d6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(\n",
    "    path: str,\n",
    "    out_csv: Optional[str] = None,\n",
    "    model: str = OLLAMA_MODEL,\n",
    "    host: str = OLLAMA_HOST,\n",
    "    use_context: bool = False,   # keep False to ignore context\n",
    ") -> pd.DataFrame:\n",
    "    objs = load_data(path)\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    # We progress per candidate\n",
    "    total_candidates = 0\n",
    "    for o in objs:\n",
    "        n = len(extract_candidates(o))\n",
    "        total_candidates += (n if n > 0 else 1)\n",
    "    if total_candidates == 0:\n",
    "        total_candidates = 1\n",
    "\n",
    "    with tqdm(total=total_candidates, desc=\"Judging answers\", unit=\"cand\") as pbar:\n",
    "        for i, obj in enumerate(objs):\n",
    "            uid = obj.get(\"id\", \"ex-{0}\".format(i))\n",
    "            context = to_context(obj)             # ignored unless use_context=True\n",
    "            targets = extract_targets(obj)\n",
    "            candidates = extract_candidates(obj)\n",
    "\n",
    "            if not targets or not candidates:\n",
    "                out_rows.append({\n",
    "                    \"id\": uid,\n",
    "                    \"candidate\": \"\",\n",
    "                    \"best_equivalent\": False,\n",
    "                    \"best_score\": 0.0,\n",
    "                    \"best_rationale\": \"Missing targets or candidates\",\n",
    "                    \"best_target\": \"\",\n",
    "                    \"context\": context,\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            for cand in candidates:\n",
    "                if pbar.n % HEARTBEAT_EVERY == 0:\n",
    "                    print(\"[heartbeat] processed {0} candidates…\".format(pbar.n), flush=True)\n",
    "\n",
    "                best = best_equivalence_against_targets(\n",
    "                    targets=targets,\n",
    "                    candidate=cand,\n",
    "                    model=model,\n",
    "                    host=host,\n",
    "                    context=context,\n",
    "                    use_context=use_context,\n",
    "                )\n",
    "                out_rows.append({\n",
    "                    \"id\": uid,\n",
    "                    \"candidate\": cand,\n",
    "                    \"best_equivalent\": best[\"best_equivalent\"],\n",
    "                    \"best_score\": best[\"best_score\"],\n",
    "                    \"best_rationale\": best[\"best_rationale\"],\n",
    "                    \"best_target\": best[\"best_target\"],\n",
    "                    \"context\": context,\n",
    "                })\n",
    "                pbar.update(1)\n",
    "\n",
    "    df = pd.DataFrame(out_rows)\n",
    "    if out_csv:\n",
    "        Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93aa809b-03fd-48ad-a4ca-a7e864aa6d31",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "squad_v2_forward.json",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m squad_v2 = \u001b[43mevaluate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msquad_v2_forward.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msquad_v2.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m trivia_qa = evaluate_file(\u001b[33m\"\u001b[39m\u001b[33mtrivia_qa_forward.json\u001b[39m\u001b[33m\"\u001b[39m, out_csv=\u001b[33m\"\u001b[39m\u001b[33mtrivia_qa.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m truthful_qa = evaluate_file(\u001b[33m\"\u001b[39m\u001b[33mtruthful_qa_forward.json\u001b[39m\u001b[33m\"\u001b[39m, out_csv=\u001b[33m\"\u001b[39m\u001b[33mtruthful_qa.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mevaluate_file\u001b[39m\u001b[34m(path, out_csv, model, host, use_context)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_file\u001b[39m(\n\u001b[32m      2\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m      3\u001b[39m     out_csv: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     use_context: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,   \u001b[38;5;66;03m# keep False to ignore context\u001b[39;00m\n\u001b[32m      7\u001b[39m ) -> pd.DataFrame:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     objs = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     out_rows: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] = []\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# We progress per candidate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      9\u001b[39m p = Path(path)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p.exists():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.suffix.lower() == \u001b[33m\"\u001b[39m\u001b[33m.jsonl\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     13\u001b[39m     rows = []\n",
      "\u001b[31mFileNotFoundError\u001b[39m: squad_v2_forward.json"
     ]
    }
   ],
   "source": [
    "squad_v2 = evaluate_file(\"squad_v2_forward.json\", out_csv=\"squad_v2.csv\")\n",
    "trivia_qa = evaluate_file(\"trivia_qa_forward.json\", out_csv=\"trivia_qa.csv\")\n",
    "truthful_qa = evaluate_file(\"truthful_qa_forward.json\", out_csv=\"truthful_qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3e12e-72cb-43e4-a141-1b4c10deed45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
